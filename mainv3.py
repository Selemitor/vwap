# mainv3.py
import multiprocessing
import pandas as pd
import numpy as np
import os
import time
import random
import optuna
import random
from pathlib import Path
from indicators import add_rsi, add_wave_trend, add_trendline_features, add_atr, add_temporal_features

from data_loader import discover_data_files, fetch_and_cache_market_caps, preload_all_data
from backtester import optimize_timeframe_group, init_backtester_worker, run_simulation
from analysis import analyze_by_market_cap
from ai_manager import TradeManagerEnv as EntryEnv, train_ai_manager as train_entry_manager
from exit_manager import ExitManagerEnv as ExitEnv, train_exit_manager

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

SCRIPT_DIR = Path(__file__).resolve().parent

# --- Definicje funkcji (pozostajƒÖ bez zmian) ---

def get_combined_data_for_training(params, files_to_use, data_store, target_timeframe=None):
    """
    Przygotowuje i ≈ÇƒÖczy dane do treningu, dodajƒÖc wszystkie potrzebne wska≈∫niki.
    """
    
    MIN_BLOCK_SIZE_FOR_INDICATORS = 150 
    processed_df_list = []
    all_dfs_raw = []
    
    if target_timeframe:
        print(f"  -> Przygotowywanie danych treningowych TYLKO dla interwa≈Çu: {target_timeframe}...")
        for pair, timeframes in files_to_use.items():
            if target_timeframe in timeframes and \
               pair in data_store and \
               target_timeframe in data_store[pair]:
                
                df_to_add = data_store[pair][target_timeframe].copy()
                if len(df_to_add) >= MIN_BLOCK_SIZE_FOR_INDICATORS:
                    all_dfs_raw.append(df_to_add)
                else:
                    print(f"  -> Pomijanie {pair} {target_timeframe} (zbyt ma≈Ço danych: {len(df_to_add)})")
    else:
        print("  -> Tryb 'Wszystkie interwa≈Çy'. Aktywowanie balansowania danych...")
        for pair, timeframes in files_to_use.items():
            for tf in timeframes:
                if pair in data_store and tf in data_store[pair]:
                    if len(data_store[pair][tf]) >= MIN_BLOCK_SIZE_FOR_INDICATORS:
                        all_dfs_raw.append(data_store[pair][tf].copy())

    if not all_dfs_raw:
        print("  -> B≈ÅƒÑD: Nie znaleziono wystarczajƒÖcych danych do po≈ÇƒÖczenia.")
        return pd.DataFrame()

    min_len = min(len(df) for df in all_dfs_raw)
    block_size = min_len
    
    if not target_timeframe: # Stosuj balansowanie tylko w trybie "Wszystkie interwa≈Çy"
        print(f"  -> Balansowanie do najkr√≥tszego zbioru: {block_size} ≈õwiec (na interwa≈Ç/parƒô).")

    print("  -> Przetwarzanie blok√≥w danych (Obliczanie wska≈∫nik√≥w PRZED ≈ÇƒÖczeniem)...")
    
    for df_original in all_dfs_raw:
        df_block = None
        if not target_timeframe: # Logika balansowania
            if len(df_original) > block_size:
                max_start_index = len(df_original) - block_size
                start_index = random.randint(0, max_start_index)
                df_block = df_original.iloc[start_index : start_index + block_size].copy()
            else:
                df_block = df_original.copy()
        else: # Tryb jednego interwa≈Çu - bierzemy wszystko
            df_block = df_original.copy()

        df_block = add_temporal_features(df_block)
        df_block = add_rsi(df_block, params.get("RSI_PERIOD", 14))
        df_block = add_wave_trend(df_block, params.get("WT_N1", 10), params.get("WT_N2", 21))
        df_block['ema_fast'] = df_block['close'].ewm(span=params["EMA_FAST"]).mean()
        df_block['ema_slow'] = df_block['close'].ewm(span=params["EMA_SLOW"]).mean()
        df_block = add_atr(df_block, period=14) 
        df_block['price_change'] = df_block['close'].pct_change()
        df_block['atr_norm'] = df_block['atr'] / df_block['close']
        df_block['signal_ema'] = df_block['close'].ewm(span=50).mean()
        df_block = add_trendline_features(df_block, lookback=72)
        
        df_block.ffill(inplace=True); df_block.bfill(inplace=True)
        processed_df_list.append(df_block)
    
    if not processed_df_list:
        print("  -> B≈ÅƒÑD: Nie znaleziono danych do po≈ÇƒÖczenia (po balansowaniu).")
        return pd.DataFrame()

    combined_df = pd.concat(processed_df_list)
    combined_df.sort_index(inplace=True)
    
    print(f"  -> Po≈ÇƒÖczone dane majƒÖ teraz {len(combined_df)} wierszy.")

    required_columns = ['close', 'ema_fast', 'ema_slow', 'RSI', 'WT1', 'WT2', 
                        'res_slope_norm', 'day_sin', 'hour_sin']
    
    if not all(col in combined_df.columns for col in required_columns):
        print("  -> OSTRZE≈ªENIE: Brak wymaganych kolumn po obliczeniu wska≈∫nik√≥w.")
        return pd.DataFrame()

    cleaned_df = combined_df.dropna(subset=required_columns)

    if cleaned_df.empty:
        print("  -> KRYTYCZNY B≈ÅƒÑD: Zbi√≥r danych treningowych jest pusty PO czyszczeniu.")

    print("  -> ‚úÖ Dane treningowe gotowe.")
    return cleaned_df

def main():
    # --- Konfiguracja i ≈Çadowanie danych ---
    files_structure_raw = discover_data_files() 

    # --- NOWY BLOK: Filtracja Stablecoin√≥w i Wrapped Tokens ---
    print("üîç Filtrowanie listy ticker√≥w...")
    # Lista bazowych walut stablecoin√≥w do zignorowania
    STABLECOINS = ['USDT', 'USDC', 'DAI', 'TUSD', 'BUSD', 'FDUSD', 'USDP', 'PYUSD', 'PAX', 'GUSD', 'USDD', 'FRAX']
    # G≈Ç√≥wne Wrapped Tokens
    BLACKLISTED_BASE_CURRENCIES = STABLECOINS + [
        'WBTC', 'WETH', 'WBNB', 'WAVAX', 'WMATIC', 'WFTM', 
        'STETH', 'RETH', 'CBETH', 'CETH', 'CDAI', 'CUSDC', 'AXLUSDC', 'SOBTC'
    ]

    files_structure = {} # Nowy, przefiltrowany s≈Çownik
    
    for ticker, timeframes in files_structure_raw.items():
        try:
            base_currency = ticker.split('_')[0].upper()
            
            if base_currency in BLACKLISTED_BASE_CURRENCIES:
                continue
                
            # Je≈õli przeszed≈Ç filtry, dodaj go
            files_structure[ticker] = timeframes
            
        except Exception as e:
            print(f"  -> B≈ÇƒÖd filtrowania {ticker}: {e}")
    
    print(f"‚úÖ Przefiltrowano. Pozosta≈Ço {len(files_structure)} z {len(files_structure_raw)} par.")
    # --- KONIEC BLOKU FILTRACJI ---

    all_tickers = list(files_structure.keys())
    market_caps = fetch_and_cache_market_caps(all_tickers)
    
    sorted_tickers = sorted(market_caps, key=market_caps.get, reverse=True)

    # --- NOWA LOGIKA WYBORU GRUP ---
    print(f"≈ÅƒÖcznie posortowanych ticker√≥w: {len(sorted_tickers)}")

    num_training = 100
    training_tickers = sorted_tickers[:num_training]

    num_testing = 50
    testing_tickers = sorted_tickers[num_training : num_training + num_testing]

    if not testing_tickers:
        print("OSTRZE≈ªENIE: Brak ticker√≥w w grupie testowej (101-200). U≈ºywam reszty ticker√≥w.")
        testing_tickers = sorted_tickers[num_training:]
    # --- KONIEC NOWEJ LOGIKI ---
        
    print("\n" + "="*50); print(f"üìä Podzia≈Ç na grupy:"); print(f"  -> Grupa Treningowa (TOP {num_training} MCap): {len(training_tickers)} par"); print(f"  -> Grupa Testowa (Miejsca 101-{101+num_testing}): {len(testing_tickers)} par"); print("="*50)
    
    tickers_to_load = list(set(training_tickers + testing_tickers)); files_structure_to_load = {k: v for k, v in files_structure.items() if k in tickers_to_load}
    print(f"\n≈ÅƒÖcznie zostanie wczytanych {len(tickers_to_load)} par (treningowe + testowe)."); data_store = preload_all_data(files_structure_to_load)

    base_params = {"SL_PCT": 0.05, "TRADE_DIRECTION": "both", "EMA_FAST": 12, "EMA_SLOW": 26, "RSI_PERIOD": 14, "WT_N1": 10, "WT_N2": 21, "USE_EMA": False}
    training_files_structure = {k: v for k, v in files_structure.items() if k in training_tickers}
    
    models_dir = SCRIPT_DIR / "Models"; models_dir.mkdir(exist_ok=True)
    
    # Nazwy modeli "Generalist"
    model_suffix = "_combined"
    entry_model_path = models_dir / f"ai_entry_strategist_model{model_suffix}.zip"
    exit_model_path = models_dir / f"ai_exit_manager_model{model_suffix}.zip"
    
    entry_model, exit_model = None, None
    
    # --- FAZA 1: Trening Modeli Og√≥lnych (Generalist) ---
    print("\n" + "="*50); print("ü§ñ FAZA 1: Trening Modeli Og√≥lnych (Generalist)..."); print("="*50 + "\n")
    
    training_data = get_combined_data_for_training(base_params, 
                                                 training_files_structure, 
                                                 data_store, 
                                                 target_timeframe=None)
    
    if training_data.empty: print("Krytyczny b≈ÇƒÖd: Brak danych do treningu."); return
    
    # Tworzymy env, kt√≥re bƒôdƒÖ u≈ºywane do ≈Çadowania modeli
    temp_entry_env = DummyVecEnv([lambda: EntryEnv(training_data, base_params)])
    temp_exit_env = DummyVecEnv([lambda: ExitEnv(training_data, base_params)])
    
    if os.path.exists(entry_model_path): 
        print(f"‚úÖ Znaleziono model wej≈õcia (Generalist). ≈Åadowanie (na CPU)..."); 
        # <<< ZMIANA: ≈Åadujemy na CPU do backtestingu >>>
        entry_model = PPO.load(entry_model_path, env=temp_entry_env, device="cpu")
    if entry_model is None: 
        print("ü§ñ Model wej≈õcia nie istnieje. Rozpoczynanie treningu...")
        entry_model = train_entry_manager(training_data, base_params, timesteps=500000, save_path=entry_model_path)
        print("‚úÖ Trening zako≈Ñczony. ≈Åadowanie modelu (na CPU) do backtestingu...")
        # <<< ZMIANA: ≈Åadujemy na CPU do backtestingu >>>
        entry_model = PPO.load(entry_model_path, env=temp_entry_env, device="cpu")
    
    if os.path.exists(exit_model_path): 
        print(f"‚úÖ Znaleziono model wyj≈õcia (Generalist). ≈Åadowanie (na CPU)..."); 
        # <<< ZMIANA: ≈Åadujemy na CPU do backtestingu >>>
        exit_model = PPO.load(exit_model_path, env=temp_exit_env, device="cpu")
    if exit_model is None: 
        print("ü§ñ Model wyj≈õcia nie istnieje. Rozpoczynanie treningu...")
        exit_model = train_exit_manager(training_data, base_params, timesteps=500000, save_path=exit_model_path)
        print("‚úÖ Trening zako≈Ñczony. ≈Åadowanie modelu (na CPU) do backtestingu...")
        # <<< ZMIANA: ≈Åadujemy na CPU do backtestingu >>>
        exit_model = PPO.load(exit_model_path, env=temp_exit_env, device="cpu")
    
    if not all([entry_model, exit_model]): print("Krytyczny b≈ÇƒÖd: Nie uda≈Ço siƒô przygotowaƒá modeli AI."); return

    all_timeframes = sorted(list(set(tf for tfs in files_structure.values() for tf in tfs)))
    
    # Przekazujemy za≈Çadowane modele "Generalist" do Optuny
    init_backtester_worker(data_store)
    
    # --- FAZA 2: Optymalizacja Parametr√≥w (Optuna) ---
    print("\n" + "="*50); print("üöÄ FAZA 2: Rozpoczynanie pƒôtli optymalizacji (u≈ºywa modeli 'Generalist' na CPU)."); print("="*50 + "\n")
    
    MAX_OPTUNA_CYCLES = 10 # <<< NOWA STA≈ÅA: Limit cykli dla fazy Optuny (Faza 2) >>>
    
    try:
        cycle_num = 0
        while cycle_num < MAX_OPTUNA_CYCLES: # Zmieniamy na limit
            print(f"\n====================== CYKL OPTUNY: {cycle_num + 1} z {MAX_OPTUNA_CYCLES} ======================")
            for timeframe in all_timeframes:
                print(f"\n--- Interwa≈Ç: {timeframe} ---")
                pairs_for_timeframe = [p for p, tfs in files_structure.items() if timeframe in tfs and p in testing_tickers]
                if not pairs_for_timeframe: 
                    print(f"Brak par z losowej grupy testowej dla interwa≈Çu {timeframe}.")
                    continue
                
                # Optuna u≈ºywa modeli "Generalist"
                optimize_timeframe_group(entry_model, exit_model, pairs_for_timeframe, timeframe, n_trials=10, storage_dir=SCRIPT_DIR)
            cycle_num += 1
            time.sleep(5)
            
        print("\n‚úÖ OsiƒÖgniƒôto limit cykli Optuny. Automatyczne przechodzenie do Fazy 3: Dostrajanie.") # Nowy komunikat
        
    except KeyboardInterrupt:
        print("\nPrzerwano pƒôtlƒô optymalizacji (Ctrl+C). Przechodzenie do Fazy 3: Dostrajanie.")
    
    # --- FAZA 3: Dostrajanie (Fine-Tuning) Modeli ---
    print("\n" + "="*50); print("ü§ñ FAZA 3: Rozpoczynanie dostrajania modeli specjalistycznych..."); print("="*50 + "\n")
    
    FINE_TUNE_TIMESTEPS_ENTRY = 250000
    FINE_TUNE_TIMESTEPS_EXIT = 250000 
    
    for timeframe in all_timeframes:
        print(f"\n--- Dostrajanie dla interwa≈Çu: {timeframe} ---")
        
        try:
            study_name = f"study_{timeframe}_group_exit_manager"
            storage_name = "postgresql://postgres:twojehaslo@localhost:5432/optuna_db"
            study = optuna.load_study(study_name=study_name, storage=storage_name)
            best_params_from_study = study.best_params
            
            current_params = {**base_params, **best_params_from_study}

            print(f"  -> Znaleziono najlepsze parametry: {best_params_from_study}")

            print(f"  -> Przygotowywanie specjalistycznych danych dla {timeframe}...")
            specialist_data = get_combined_data_for_training(
                current_params,
                training_files_structure, 
                data_store, 
                target_timeframe=timeframe
            )
            
            if specialist_data.empty:
                print(f"  -> B≈ÅƒÑD: Brak danych do dostrojenia dla {timeframe}. Pomijanie.")
                continue
            
            # 3. Dostr√≥j i zapisz model WEJ≈öCIA
            print(f"  -> Dostrajam model WEJ≈öCIA (Generalist) -> (na GPU)...")
            specialist_entry_env = DummyVecEnv([lambda: EntryEnv(specialist_data, current_params)])
            entry_model_generalist = PPO.load(entry_model_path, env=specialist_entry_env) 
            entry_model_generalist.learn(total_timesteps=FINE_TUNE_TIMESTEPS_ENTRY) # Trenujemy dalej (na GPU)
            
            specialist_entry_path = models_dir / f"ai_entry_strategist_model_specialist_{timeframe}.zip"
            entry_model_generalist.save(specialist_entry_path)
            print(f"  -> ‚úÖ Zapisano model wej≈õcia: {specialist_entry_path.name}")

            # 4. Dostr√≥j i zapisz model WYJ≈öCIA
            print(f"  -> Dostrajam model WYJ≈öCIA (Generalist) -> (na GPU)...")
            specialist_exit_env = DummyVecEnv([lambda: ExitEnv(specialist_data, current_params)])
            exit_model_generalist = PPO.load(exit_model_path, env=specialist_exit_env) 
            exit_model_generalist.learn(total_timesteps=FINE_TUNE_TIMESTEPS_EXIT) # Trenujemy dalej (na GPU)

            specialist_exit_path = models_dir / f"ai_exit_manager_model_specialist_{timeframe}.zip"
            exit_model_generalist.save(specialist_exit_path)
            print(f"  -> ‚úÖ Zapisano model wyj≈õcia: {specialist_exit_path.name}")

        except Exception as e:
            print(f"‚ùå B≈ÇƒÖd podczas dostrajania dla {timeframe}: {e}")

    # --- FAZA 4: Ko≈Ñcowa Analiza (z u≈ºyciem Modeli Specjalistycznych) ---
    print("\n" + "="*50); print("üìä FAZA 4: Ko≈Ñcowa analiza z u≈ºyciem modeli SPECJALISTYCZNYCH..."); print("="*50 + "\n")
    
    all_trades_list = []
    try:
        for timeframe in all_timeframes:
            pairs_for_timeframe = [p for p, tfs in files_structure.items() if timeframe in tfs and p in testing_tickers]
            if not pairs_for_timeframe: continue
            
            print(f"\n--- Analiza ko≈Ñcowa dla: {timeframe} ---")
            
            try:
                study_name = f"study_{timeframe}_group_exit_manager"
                storage_name = "postgresql://postgres:twojehaslo@localhost:5432/optuna_db"
                study = optuna.load_study(study_name=study_name, storage=storage_name)
                best_params = study.best_params
                
                specialist_entry_path = models_dir / f"ai_entry_strategist_model_specialist_{timeframe}.zip"
                specialist_exit_path = models_dir / f"ai_exit_manager_model_specialist_{timeframe}.zip"

                if not specialist_entry_path.exists() or not specialist_exit_path.exists():
                    print(f"  -> OSTRZE≈ªENIE: Brak modeli specjalistycznych dla {timeframe}. Pomijanie analizy.")
                    continue
                    
                print(f"  -> ≈Åadowanie modeli specjalistycznych dla {timeframe} (na CPU)...")
                
                entry_model_specialist = PPO.load(specialist_entry_path, env=temp_entry_env, device="cpu")
                exit_model_specialist = PPO.load(specialist_exit_path, env=temp_exit_env, device="cpu")
                
                print(f"  -> Uruchamianie symulacji na grupie testowej (z modelami specjalistycznymi na CPU)...")
                
                for pair in pairs_for_timeframe:
                    sim_result = run_simulation(
                        best_params, 
                        pair, 
                        timeframe, 
                        entry_model_specialist,  
                        exit_model_specialist    
                    )
                    all_trades_list.extend(sim_result['trades'])
                    
            except Exception as e:
                print(f"B≈ÇƒÖd wczytywania lub ponownego uruchamiania symulacji dla {timeframe}: {e}")
        
        if not all_trades_list:
            print("Nie zebrano ≈ºadnych transakcji do ko≈Ñcowej analizy.")
            return

        print(f"‚úÖ Zebrano ≈ÇƒÖcznie {len(all_trades_list)} transakcji do analizy.")
        
        results_df = pd.DataFrame(all_trades_list)
        results_df.dropna(subset=['exit_price', 'avg_price'], inplace=True)

        if results_df.empty:
            print("Brak zako≈Ñczonych transakcji do analizy.")
            return

        results_df['PnL'] = 0.0
        longs = results_df['side'] == 'long'
        shorts = results_df['side'] == 'short'
        
        results_df.loc[longs, 'PnL'] = ((results_df['exit_price'] - results_df['avg_price']) / results_df['avg_price']) * 100
        results_df.loc[shorts, 'PnL'] = ((results_df['avg_price'] - results_df['exit_price']) / results_df['avg_price']) * 100
        
        analyze_by_market_cap(results_df, market_caps)

    except Exception as e:
        print(f"Krytyczny b≈ÇƒÖd w Fazie 4 (Analiza): {e}")

if __name__ == "__main__":
    multiprocessing.freeze_support()
    main()
